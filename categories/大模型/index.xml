<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大模型 on Yuta&#39;s blog</title>
        <link>https://h-yx-blog.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
        <description>Recent content in 大模型 on Yuta&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Yuta Huang</copyright>
        <lastBuildDate>Sun, 09 Feb 2025 10:08:24 +0800</lastBuildDate><atom:link href="https://h-yx-blog.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>DeepSeek Janus Pro</title>
        <link>https://h-yx-blog.github.io/p/deepseek-janus-pro/</link>
        <pubDate>Sun, 09 Feb 2025 10:08:24 +0800</pubDate>
        
        <guid>https://h-yx-blog.github.io/p/deepseek-janus-pro/</guid>
        <description>&lt;h1 id=&#34;一介绍&#34;&gt;一.介绍
&lt;/h1&gt;&lt;p&gt;Janus 是一种新颖的自回归框架，它统一了多模态理解和生成。它通过将视觉编码解耦为单独的路径来解决以前方法的局限性，同时仍然使用单一、统一的 transformer 架构进行处理。解耦不仅缓解了视觉编码器在理解和生成中的角色冲突，还增强了框架的灵活性。Janus 超越了以前的统一模型，并达到或超过特定于任务的模型的性能。Janus 的简单性、高度灵活性和有效性使其成为下一代统一多模态模型的有力候选者。&lt;/p&gt;
&lt;h1 id=&#34;二关键技术&#34;&gt;二.关键技术
&lt;/h1&gt;&lt;p&gt;Janus-Pro 是 DeepSeek 的 Janus 多模态模型的高级版本，设计用于出色地理解和生成涉及文本和图像的内容，&lt;strong&gt;可以同时进行多模态理解和图像生成任务。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Janus-Pro 仍然保持了 Janus 首创的解耦视觉编码这一核心架构原则。这种将不同任务的视觉处理方法分离开来的做法，对于提高模型的整体性能至关重要。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化训练策略 ：Janus-Pro 采用更有效的训练策略，注重更好地利用数据和资源。&lt;/li&gt;
&lt;li&gt;扩展的训练数据集：该模型结合了真实数据源和合成数据，增强了其稳健性和适应性。&lt;/li&gt;
&lt;li&gt;更大的模型规模：Janus-Pro 的参数规模从 10 亿 (1B) 到 70 亿 (7B)，性能和稳定性都得到了提高，尤其是在文本到图像生成和多模态理解等任务中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;三创新&#34;&gt;三.创新
&lt;/h1&gt;&lt;p&gt;核心创新是将&lt;strong&gt;多模态理解和生成&lt;/strong&gt;，使用&lt;strong&gt;两种不同的任务解耦编码器&lt;/strong&gt;来完成理解和生成任务。让这两种任务可以在同一模型中高效完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解耦视觉编码器&lt;/strong&gt;：
Janus-Pro 架构的核心是解耦视觉编码的理念，即根据手头的任务使用不同的编码方法处理图像。
这种多任务处理的效果，主要源于 Janus 系列针对不同任务使用的两个视觉编码器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;理解编码器：用于提取图像中的语义特征，以便进行图像理解任务（如图像问答、视觉分类等）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成编码器：将图像转换为离散表示（例如，使用 VQ 编码器），用于文本到图像生成任务。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;文生图对比&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;特性&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Janus Pro&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Flux&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;训练成本&lt;/td&gt;
          &lt;td&gt;相对较低的预算&lt;/td&gt;
          &lt;td&gt;未明确说明，可能更高&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;社区支持&lt;/td&gt;
          &lt;td&gt;开源，在 Hugging Face 上可用&lt;/td&gt;
          &lt;td&gt;拥有强大的社区支持和优化&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;性能&lt;/td&gt;
          &lt;td&gt;擅长指令执行，多模态任务&lt;/td&gt;
          &lt;td&gt;高质量图像且生成速度快&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;图像分辨率&lt;/td&gt;
          &lt;td&gt;输入：384 x 384 像素，输出：最高 768 x 768&lt;/td&gt;
          &lt;td&gt;可生成高达 1024 x 1024 像素&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;主要关注点&lt;/td&gt;
          &lt;td&gt;多模态任务，文本-图像交互&lt;/td&gt;
          &lt;td&gt;高质量图像生成&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;四评估结果&#34;&gt;四.评估结果
&lt;/h1&gt;&lt;p&gt;①在 GenEval 中，Janus-Pro-7B 的得分为 0.80，超过了 DALL-E 3 和 Stable Diffusion 3 Medium 等模型。&lt;/p&gt;
&lt;p&gt;②在 DPG-Bench 上，Janus-Pro 获得了 84.19 分，再次超过了 DALL-E 3 和 Stable Diffusion 3 Medium 等竞争对手。
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/result.jpg&#34;
	width=&#34;851&#34;
	height=&#34;659&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/result_hu_d9d177b7098ed4e7.jpg 480w, https://h-yx-blog.github.io/p/deepseek-janus-pro/result_hu_eb76ea1fe37ccc5b.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;评估结果对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;309px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;五局限性&#34;&gt;五.局限性
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;分辨率限制：该模型的输入分辨率上限为 384x384，这可能会妨碍需要精细细节的任务（如光学字符识别 (OCR)）的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;图像质量：同样的分辨率限制和 VQ tokenizer 使用的压缩技术会导致图像缺乏精细度，尤其是面部小区域。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;六-本地部署&#34;&gt;六. 本地部署
&lt;/h1&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#创建conda环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;conda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;create&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;deepseek&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;janus&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;python&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.10&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#激活conda环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;conda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;activate&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;deepseek&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;janus&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#克隆Janus项目&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;git&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;clone&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;github&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;com&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;deepseek&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ai&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Janus&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;git&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#进入janus目录&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Janus&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#安装Janus依赖&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#安装Gradio （UI界面）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gradio&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#启动 （代码中默认是Janus pro 7B的模型，可以手动修改）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;python&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;demo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;app_januspro&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;py&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;看到以下界面即已经成功部署
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/deploy.jpg&#34;
	width=&#34;1667&#34;
	height=&#34;1123&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/deploy_hu_67b5360133567b06.jpg 480w, https://h-yx-blog.github.io/p/deepseek-janus-pro/deploy_hu_cfd9310ad4a15f54.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;148&#34;
		data-flex-basis=&#34;356px&#34;
	
&gt;
测试结果
①文生图测试如下
文生图功能似乎对中文理解似乎不行，只能使用英文  （使用中文提问时，无论怎么提问，总是生成如下图片）
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopage.jpg&#34;
	width=&#34;1985&#34;
	height=&#34;881&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopage_hu_92886cbb83546bf5.jpg 480w, https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopage_hu_1843926421f64584.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;中文文生图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;540px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;英文提问如下
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopagebyen.jpg&#34;
	width=&#34;1921&#34;
	height=&#34;917&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopagebyen_hu_68158571c0830003.jpg 480w, https://h-yx-blog.github.io/p/deepseek-janus-pro/texttopagebyen_hu_b5180c3b01f3c5bc.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;英文文生图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;502px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;②图片理解测试如下
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/lijie.jpg&#34;
	width=&#34;1991&#34;
	height=&#34;741&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-janus-pro/lijie_hu_eb00f814b357bbca.jpg 480w, https://h-yx-blog.github.io/p/deepseek-janus-pro/lijie_hu_b8c6fe76cd47d451.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;644px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DeepSeek R1</title>
        <link>https://h-yx-blog.github.io/p/deepseek-r1/</link>
        <pubDate>Sun, 09 Feb 2025 10:08:24 +0800</pubDate>
        
        <guid>https://h-yx-blog.github.io/p/deepseek-r1/</guid>
        <description>&lt;h1 id=&#34;一-介绍&#34;&gt;一. 介绍
&lt;/h1&gt;&lt;p&gt;1.1 DeepSeek-R1 通过强化学习（Reinforcement Learning：简称RL）提升大型语言模型（LLM）的推理能力。这项研究在如何仅依靠强化学习而不是过分依赖监督式微调的情况下，增强LLM解决复杂问题的能力上，取得了重要进展。&lt;/p&gt;
&lt;p&gt;1.2 DeepSeek-R1系列包含两大核心成员：&lt;/p&gt;
&lt;p&gt;①DeepSeek-R1-Zero&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参数规模：6710亿（MoE架构，每个token激活370亿参数）&lt;/li&gt;
&lt;li&gt;训练特点：完全基于强化学习的端到端训练&lt;/li&gt;
&lt;li&gt;核心优势：展现出自我验证、长链推理等涌现能力&lt;/li&gt;
&lt;li&gt;典型表现：AIME 2024基准测试71%准确率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;②DeepSeek-R1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参数规模：与Zero版保持相同体量&lt;/li&gt;
&lt;li&gt;训练创新：多阶段混合训练策略&lt;/li&gt;
&lt;li&gt;核心改进：监督微调冷启动 + 强化学习优化&lt;/li&gt;
&lt;li&gt;性能提升：AIME 2024准确率提升至79.8%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1.3 对比R1 Zero和R1&lt;/p&gt;
&lt;p&gt;① R1 Zero和R1的主要区别&lt;/p&gt;
&lt;p&gt;DeepSeek-R1-Zero是团队初步尝试仅用纯强化学习而不进行任何监督式微调的实验。他们从基础模型出发，直接运用强化学习，让模型通过不断试错来发展其推理能力。这种方法虽然取得了较好的成果（在 AIME 2024 测试中达到了 71% 的准确率），但在可读性和语言连贯性上存在明显不足。该模型拥有 6710 亿个参数，使用了混合专家（MoE）架构，其中每个词触发的参数约为 370 亿。此模型展现了一些新兴的推理行为，例如自我核查、反思和长链推理（CoT）。&lt;/p&gt;
&lt;p&gt;与之对比，DeepSeek-R1采用了更复杂的多阶段训练方法。它不仅仅采用强化学习，而是先在一小组精心挑选的示例（称为“冷启动数据”）上进行监督式微调，然后再应用强化学习。这种方法克服了 DeepSeek-R1-Zero 的局限，同时取得了更优的表现。这个模型同样维持了 6710 亿的参数数量，但在回答的可读性和条理性上有所提高。&lt;/p&gt;
&lt;p&gt;② R1 Zero和R1的共同点
DeepSeek-R1-Zero 和 DeepSeek-R1 基于 DeepSeek-V3-Base 进行训练。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Total Params&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Activated Params&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;DeepSeek-R1-Zero&lt;/td&gt;
          &lt;td&gt;671B&lt;/td&gt;
          &lt;td&gt;37B&lt;/td&gt;
          &lt;td&gt;128K&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;DeepSeek-R1&lt;/td&gt;
          &lt;td&gt;671B&lt;/td&gt;
          &lt;td&gt;37B&lt;/td&gt;
          &lt;td&gt;128K&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;训练方法概述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;强化学习&lt;/strong&gt; ：不同于传统依赖监督学习的模型，DeepSeek-R1 大规模采用了强化学习。此训练方法利用群体相对策略优化（GRPO），重点提升精度和格式化奖励，以增强推理能力，无需依赖大量标注数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;蒸馏技术&lt;/strong&gt; ：为普及高效能模型，DeepSeek 也推出了 R1 的蒸馏版本，参数规模从15亿到700亿不等。这些模型采用了如Qwen和Llama等架构，表明即使是较小和更高效的模型也能包含复杂的推理能力。蒸馏过程通过使用 DeepSeek-R1 生成的合成推理数据对这些小型模型进行微调，以较低的计算成本保持高性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;二优点&#34;&gt;二.优点
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;性能与 OpenAI-o1 相当&lt;/li&gt;
&lt;li&gt;完全开源的模型和技术报告&lt;/li&gt;
&lt;li&gt;MIT 许可：自由蒸馏和商业化！&lt;/li&gt;
&lt;li&gt;从 DeepSeek-R1 提炼出来，6 个小模型完全开源（32B &amp;amp; 70B 型号与 OpenAI-o1-mini 相当）&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;三-技术亮点&#34;&gt;三. 技术亮点
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;以最少的标记数据显著提高性能&lt;/li&gt;
&lt;li&gt;组相对策略优化（GRPO）：兼顾格式与准确性的奖励机制&lt;/li&gt;
&lt;li&gt;知识蒸馏技术：支持从1.5B到70B的参数规模适配&lt;/li&gt;
&lt;li&gt;多架构兼容：基于Qwen/Llama等主流架构的轻量化版本&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;四api&#34;&gt;四.API
&lt;/h1&gt;&lt;p&gt;成本优势：缓存命中时输入token成本$0.14/M，较同类产品降低30%&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;上下文长度&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;最大输出长度&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;输入价格（缓存命中）&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;输入价格（缓存未命中）&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;输出价格&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;deepseek-reasoner（即deppseek R1）&lt;/td&gt;
          &lt;td&gt;64K&lt;/td&gt;
          &lt;td&gt;8K&lt;/td&gt;
          &lt;td&gt;0.14 美元/百万tokens&lt;/td&gt;
          &lt;td&gt;0.55 美元/百万tokens&lt;/td&gt;
          &lt;td&gt;2.19 美元/百万tokens&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;五本地部署&#34;&gt;五.本地部署
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;推荐配置：NVIDIA GPU（推荐使用具有大量视频内存（VRAM）的 GPU，如：RTX 3090或更高） + 32GB内存 + 50GB存储空间&lt;/li&gt;
&lt;li&gt;最低配置：CPU（支持AVX2指令集） + 8GB内存 + 12GB存储
（能耗注意：32B+ 模型需高功率电源（1000W+）和散热系统。）
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-r1/deploy.jpg&#34;
	width=&#34;1517&#34;
	height=&#34;459&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-r1/deploy_hu_77329b18801eef2a.jpg 480w, https://h-yx-blog.github.io/p/deepseek-r1/deploy_hu_316908321735051e.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;本地部署&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;330&#34;
		data-flex-basis=&#34;793px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;建议在使用 DeepSeek-R1 系列模型时遵循以下配置
①将temperature 设置在 0.5-0.7 （推荐 0.6） 的范围内，以防止无休止的重复或不连贯的输出。&lt;/p&gt;
&lt;p&gt;②避免添加系统提示符;所有说明都应包含在用户提示符中。&lt;/p&gt;
&lt;p&gt;③对于数学问题，建议在提示中包含一条指令，例如：“请逐步推理，并将您的最终答案放在 \boxed{} 中。&lt;/p&gt;
&lt;p&gt;④在评估模型性能时，建议进行多次测试并平均结果。&lt;/p&gt;
&lt;h1 id=&#34;六-deepseek-r1评估结果&#34;&gt;六. DeepSeek-R1评估结果
&lt;/h1&gt;&lt;p&gt;DeepSeek-R1各方面的评估结果如下&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-r1/result.jpg&#34;
	width=&#34;720&#34;
	height=&#34;720&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-r1/result_hu_a89aecc479b790b6.jpg 480w, https://h-yx-blog.github.io/p/deepseek-r1/result_hu_5970b5d2e4c2e7d7.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;结果对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;240px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;DeepSeek-R1在数学、代码和推理任务，性能可与 OpenAI-o1 相媲美
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-r1/resulttwo.jpg&#34;
	width=&#34;895&#34;
	height=&#34;533&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-r1/resulttwo_hu_d8f61149d3ccdfba.jpg 480w, https://h-yx-blog.github.io/p/deepseek-r1/resulttwo_hu_b3a84b0de6a37aa4.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;结果对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;167&#34;
		data-flex-basis=&#34;403px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;蒸馏小模型32B 和 70B 模型在多项能力上实现了对标 OpenAI o1-mini 的效果。
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek-r1/resultthree.jpg&#34;
	width=&#34;1171&#34;
	height=&#34;563&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek-r1/resultthree_hu_d14261896c250f08.jpg 480w, https://h-yx-blog.github.io/p/deepseek-r1/resultthree_hu_1ad36f21ad07e101.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;结果对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;499px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;code220000datasuccessdatasourcetextchannel-coded-channel-scrambled--content-scrambledn-please-check-if-the-ci--module-and-the-smart-card-have-been-inserted-correctlytargettextcanale-codificato-canale-criptato--contenuto-criptaton-si-prega-di-verificare-se-il-modulo-ci-e-la-smart-card-sono-stati-inseriti-correttamentesourcetextthe-input-doesnt-support-auto-scantargettextlingresso-non-supporta-la-scansione-automaticasourcetext-targettext-sourcetexttargettextlanguageitalianfaileddatamessagesuccess七局限性及未来发展&#34;&gt;{&amp;ldquo;code&amp;rdquo;:220000,&amp;ldquo;data&amp;rdquo;:{&amp;ldquo;successData&amp;rdquo;:[{&amp;ldquo;sourceText&amp;rdquo;:&amp;ldquo;Channel coded. (Channel scrambled \ Content scrambled).\n Please check if the CI + module and the smart card have been inserted correctly.&amp;rdquo;,&amp;ldquo;targetText&amp;rdquo;:&amp;ldquo;Canale codificato. (Canale criptato \ Contenuto criptato).\n Si prega di verificare se il modulo CI+ e la smart card sono stati inseriti correttamente.&amp;rdquo;},{&amp;ldquo;sourceText&amp;rdquo;:&amp;ldquo;The input doesn\&amp;rsquo;t support auto-scan&amp;rdquo;,&amp;ldquo;targetText&amp;rdquo;:&amp;ldquo;L&amp;rsquo;ingresso non supporta la scansione automatica&amp;rdquo;},{&amp;ldquo;sourceText&amp;rdquo;:&amp;quot;-&amp;quot;,&amp;ldquo;targetText&amp;rdquo;:&amp;quot;-&amp;quot;},{&amp;ldquo;sourceText&amp;rdquo;:&amp;quot;.&amp;quot;,&amp;ldquo;targetText&amp;rdquo;:&amp;quot;.&amp;quot;}],&amp;ldquo;language&amp;rdquo;:&amp;ldquo;Italian&amp;rdquo;,&amp;ldquo;failedData&amp;rdquo;:[]},&amp;ldquo;message&amp;rdquo;:&amp;ldquo;success&amp;rdquo;}七.局限性及未来发展
&lt;/h1&gt;&lt;p&gt;若干改进领域：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型在处理需要特定输出格式的任务时偶尔会遇到困难。&lt;/li&gt;
&lt;li&gt;软件工程相关任务的性能还有提升空间。&lt;/li&gt;
&lt;li&gt;在多语言环境下，语言混合带来了挑战。&lt;/li&gt;
&lt;li&gt;少样本提示通常会导致性能下降。
未来的研究将致力于解决这些问题，并拓展模型在函数调用、多轮交互和复杂角色扮演场景等领域的能力。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>DeepSeek技术调研及api接入</title>
        <link>https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/</link>
        <pubDate>Sun, 09 Feb 2025 09:55:44 +0800</pubDate>
        
        <guid>https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/</guid>
        <description>&lt;h1 id=&#34;一-简介&#34;&gt;一. 简介
&lt;/h1&gt;&lt;p&gt;1.1  介绍：DeepSeek 是一家专注于人工智能技术研发的公司，致力于打造高性能、低成本的 AI 模型。它的目标是让 AI 技术更加普惠，让更多人能够用上强大的 AI 工具。&lt;/p&gt;
&lt;p&gt;1.2 为什么DeepSeek-V3重要&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源精神：DeepSeek-V3 不仅开源了模型权重，还提供了本地部署的支持，让开发者可以自由定制和优化模型。&lt;/li&gt;
&lt;li&gt;普惠 AI：DeepSeek-V3 的价格非常亲民，相比国外模型（如 GPT-4o），它的使用成本更低，适合中小企业和个人开发者。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;二-收费情况&#34;&gt;二. 收费情况
&lt;/h1&gt;&lt;p&gt;2.1 API收费情况
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/shoufei.jpg&#34;
	width=&#34;1241&#34;
	height=&#34;305&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/shoufei_hu_c6652b261f61e108.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/shoufei_hu_b16bd673f08d026a.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;API收费&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;406&#34;
		data-flex-basis=&#34;976px&#34;
	
&gt;
1.如未指定 max_tokens，默认最大输出长度为 4K。请调整 max_tokens 以支持更长的输出。&lt;/p&gt;
&lt;p&gt;2.表格中展示了优惠前与优惠后的价格。即日起至北京时间 2025-02-08 24:00，所有用户均可享受 DeepSeek-V3 API 的价格优惠。 在此之后，模型价格将恢复至原价。&lt;/p&gt;
&lt;p&gt;在大模型 API 的使用场景中，用户的输入有相当比例是重复的。举例说，用户的 prompt 往往有一些重复引用的部分；再举例说，多轮对话中，每一轮都要将前几轮的内容重复输入。&lt;/p&gt;
&lt;p&gt;为此，&lt;strong&gt;DeepSeek 启用上下文硬盘缓存技术，把预计未来会重复使用的内容，缓存在分布式的硬盘阵列中。如果输入存在重复，则重复的部分只需要从缓存读取，无需计算。该技术不仅降低服务的延迟，还大幅削减最终的使用成本。&lt;/strong&gt; 缓存命中的部分，DeepSeek 收费 0.1元 每百万 tokens。至此，大模型的价格再降低一个数量级&lt;/p&gt;
&lt;p&gt;Token 是模型用来表示自然语言文本的基本单位，也是我们的计费单元，可以直观的理解为“字”或“词”；通常 1 个中文词语、1 个英文单词、1 个数字或 1 个符号计为 1 个 token。&lt;/p&gt;
&lt;p&gt;一般情况下模型中 token 和字数的换算比例大致如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 个英文字符 ≈ 0.3 个 token。&lt;/li&gt;
&lt;li&gt;1 个中文字符 ≈ 0.6 个 token。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2.2 并发&lt;/p&gt;
&lt;p&gt;DeepSeek API 不限制用户并发量&lt;/p&gt;
&lt;p&gt;但请注意，当DeepSeek服务器承受高流量压力时，您的请求发出后，可能需要等待一段时间才能获取服务器的响应。在这段时间里，您的 HTTP 请求会保持连接，并持续收到如下格式的返回内容：&lt;/p&gt;
&lt;p&gt;非流式请求：持续返回空行
流式请求：持续返回 SSE keep-alive 注释（: keep-alive）&lt;/p&gt;
&lt;p&gt;这些内容不影响 OpenAI SDK 对响应的 JSON body 的解析。如果使用者自己解析 HTTP 响应，请注意处理这些空行或注释。&lt;/p&gt;
&lt;h1 id=&#34;三-接入方式&#34;&gt;三. 接入方式
&lt;/h1&gt;&lt;p&gt;3.1 API接入&lt;/p&gt;
&lt;p&gt;对话API ：https://api.deepseek.com/chat/completions&lt;/p&gt;
&lt;p&gt;请求头  Authorization ： Bearer  &lt;Token&gt;&lt;/p&gt;
&lt;p&gt;请求体， messages参数和model参数是必填的，参数示例如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;messages&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nt&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;You are a helpful assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nt&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nt&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;帮我翻译我是一个机器人，你是一个真人&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;nt&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;deepseek-chat&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;请求参数详情
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/requestparams.jpg&#34;
	width=&#34;975&#34;
	height=&#34;477&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/requestparams_hu_bb678193e04964c1.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/requestparams_hu_fa5878730f9e407.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;请求参数详情&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;204&#34;
		data-flex-basis=&#34;490px&#34;
	
&gt;
响应示例
①非流式
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/notstream.jpg&#34;
	width=&#34;1077&#34;
	height=&#34;1039&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/notstream_hu_51879ee0f409b84d.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/notstream_hu_ad1bd89d9756a144.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;非流式&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;103&#34;
		data-flex-basis=&#34;248px&#34;
	
&gt;
②流式
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/stream.jpg&#34;
	width=&#34;1775&#34;
	height=&#34;1123&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/stream_hu_1f137809796d31ca.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/stream_hu_41df2ab473689015.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;流式&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;379px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;3.2 错误码
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/errorcode.jpg&#34;
	width=&#34;825&#34;
	height=&#34;545&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/errorcode_hu_5201d972753f9de9.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/errorcode_hu_bfddc3eeb81658cf.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;错误码&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;363px&#34;
	
&gt;
3.3 与其他大模型的对比
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compareapi.jpg&#34;
	width=&#34;1837&#34;
	height=&#34;405&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compareapi_hu_784c09bd74238a66.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compareapi_hu_c9d6f3884da01b5.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;对比其他大模型&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;453&#34;
		data-flex-basis=&#34;1088px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;四对比&#34;&gt;四.对比
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;知识类任务：MMLU，MMLU-Pro， GPQA, SimpleQA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;长文本：DROP，FRAMES ，LongBench v2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;代码：算法类（Codeforces），工程类（SWE-Bench Verified）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数学：AIME 2024, MATH
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compare.jpg&#34;
	width=&#34;3521&#34;
	height=&#34;2763&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compare_hu_25a9169e0ef097eb.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/compare_hu_7a78adec988daaac.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;305px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DeepSeek-V3 多项评测成绩超越了 Qwen2.5-72B 和 Llama-3.1-405B 等其他开源模型，并在性能上和世界顶尖的闭源模型 GPT-4o 以及 Claude-3.5-Sonnet 不分伯仲。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;百科知识： DeepSeek-V3 在知识类任务的水平接近当前表现最好的模型 Claude-3.5-Sonnet-1022。
长文本： 在长文本测评中，DeepSeek-V3 平均表现超越其他模型。&lt;/li&gt;
&lt;li&gt;代码： DeepSeek-V3 在算法类代码场景远远领先于市面上已有的全部非 o1 类模型；并在工程类代码场景（SWE-Bench Verified）逼近 Claude-3.5-Sonnet-1022。&lt;/li&gt;
&lt;li&gt;数学： 在美国数学竞赛和全国高中数学联赛上，DeepSeek-V3 大幅超过了所有开源闭源模型。&lt;/li&gt;
&lt;li&gt;中文能力： DeepSeek-V3 与 Qwen2.5-72B 在教育类测评 C-Eval 和代词消歧等评测集上表现相近，但在事实知识 C-SimpleQA 上更为领先。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DeepSeek-V3支持本地部署，GPT-4o和Kimi均不支持本地部署
&lt;img src=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/comparegpt.jpg&#34;
	width=&#34;1135&#34;
	height=&#34;511&#34;
	srcset=&#34;https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/comparegpt_hu_6c14446cc1bed6db.jpg 480w, https://h-yx-blog.github.io/p/deepseek%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E5%8F%8Aapi%E6%8E%A5%E5%85%A5/comparegpt_hu_5757ac454e55c868.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;对比gpt4o&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;222&#34;
		data-flex-basis=&#34;533px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;五-创新点&#34;&gt;五. 创新点
&lt;/h1&gt;&lt;p&gt;使用上下文硬盘缓存技术，将预计未来会重复使用的内容进行缓存，如果输入存在重复，则重复的部分只需要从缓存读取，无需计算。该技术不仅降低服务的延迟，还大幅削减最终的使用成本。&lt;/p&gt;
&lt;p&gt;注意，只有当两个请求的前缀内容相同时（从第 0 个 token 开始相同），才算重复。中间开始的重复不能被缓存命中。&lt;/p&gt;
&lt;p&gt;使用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;具有长预设提示词的问答助手类应用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;具有长角色设定与多轮对话的角色扮演类应用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;针对固定文本集合进行频繁询问的数据分析类应用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;代码仓库级别的代码分析与排障工具&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过 Few-shot 提升模型输出效果
注意：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缓存系统以 64 tokens 为一个存储单元，不足 64 tokens 的内容不会被缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缓存系统是“尽力而为”，不保证 100% 缓存命中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缓存不再使用后会自动被清空，时间一般为几个小时到几天&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;六-总结&#34;&gt;六. 总结
&lt;/h1&gt;&lt;p&gt;DeepSeek-V3 是一款性能强大、价格亲民、开源支持的国产 AI 模型。它在知识问答、长文本处理、代码生成、数学能力等方面都展现出了与国际顶尖模型（如 GPT-4o）不相上下的实力。同时，它的低成本和开源特性让它成为普惠 AI 的典范。 未来，随着 DeepSeek-V3 的不断优化和功能扩展，它有望在更多领域发挥重要作用，成为国产 AI 技术的标杆。无论是企业还是个人开发者，都可以通过 DeepSeek-V3 享受到高性能、低成本的 AI 服务。 &lt;/p&gt;
&lt;p&gt;未来发展方向&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态支持&lt;/strong&gt; ：DeepSeek 计划在未来为 V3 模型添加多模态功能（如图像、音频处理），进一步提升模型的实用性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度思考能力&lt;/strong&gt; ：DeepSeek 将继续优化模型的推理和思考能力，使其能够处理更复杂的任务。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
